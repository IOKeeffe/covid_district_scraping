# this is the original script i used to scrape all the homepages of district sites
# it's kind of ugly! but you might want to reference this if things get messed up with scrape_002

# to figure out what urls need to be changed
# I didn't want to rescrape the same page over n over n over and risk getting my IP banned
# thus I made a new loop every time I needed to fix or remove a url
# but now that all the urls have been fixed or removed as necessary
# the thing should work with just the first loop!

# the loop might randomly stop on a url; sometimes, the loop just needs to be run again to fix it

library(here)
library(rvest) # xml2
library(tidyverse) # ggplot2, tibble, tidyr, readr, purrr, dplyr, stringr, forcats

#### MESS WITH DATA ####

# read in data
data <- read_csv(here("data", "magnet_project.csv"))

# get district info for districts that have webpages + add magnet column
district_info <- data %>%
  select(-c(`School Number`, `School Name`)) %>%
  rename(district.num = `District Number`,
         district.name = `District Name`,
         district.page = `District Web Page Address`) %>%
  distinct() %>%
  filter(!is.na(district.page))
  # %>% mutate(magnet = 0)
  
# remove the ' from district numbers
district_info$district.num <- str_replace(district_info$district.num, "'", "")
district_info <- district_info %>%
  mutate(district.num = as.numeric(district.num))

# DEAL WITH  WONKY SITES - mutate to fix sites, filter to remove ones that don't work
district_info <- district_info %>%
  mutate(district.page = 
           case_when(district.num == 30901 ~ "www.crossplainsisd.net", # Cross Plains ISD
                     district.num == 31913 ~ "www.smisd.net", # Santa Maria ISD
                     district.num == 37908 ~ "www.newsummerfieldisd.org", # New Summerfield ISD
                     district.num == 39905 ~ "www.midwayisd.org", # Midway ISD
                     district.num == 57848 ~ "www.iltexas.org", # International Leadership of Texas
                     district.num == 61908 ~ "www.sangerisd.net", # Sanger ISD
                     district.num == 72910 ~ "www.mmisd.us", # Morgan Mill ISD
                     district.num == 90905 ~ "www.gvhisd.net", # Grandview-Hopkins ISD
                     district.num == 101872 ~ "etoileacademy.org", # Etoile Academy Charter School
                     district.num == 101917 ~ "www1.pasadenaisd.org/",  # Pasadena ISD
                     district.num == 108808 ~ "www.vanguardacademy.education", # Vanguard Academy
                     district.num == 108905 ~ "www.hidalgo-isd.org", # Hidalgo ISD
                     district.num == 108910 ~ "progreso.schooldesk.net", # Progreso ISD
                     district.num == 110901 ~ "www.antonisd.org", # Anton ISD
                     district.num == 115901 ~ "fhisd.ss7.sharpschool.com/", # FT Hancock ISD
                     district.num == 116901 ~ "www.caddomillsisd.org", # Caddo Mills ISD
                     district.num == 116916 ~ "www.bolesisd.com", # Boles ISD
                     district.num == 144903 ~ "www.dimeboxisd.net", # Dime Box ISD
                     district.num == 153905 ~ "www.newhomeisd.org", # New Home ISD
                     district.num == 153907 ~ "www.wilson.esc17.net", # Wilson ISD
                     district.num == 156905 ~ "www.gradyisd.org", # Grady ISD
                     district.num == 161801 ~ "www.eoacwaco.org/waco-charter-school/", # Waco Charter School
                     district.num == 166901 ~ "www.cameronisd.net", # Cameron ISD
                     district.num == 172902 ~ "www.dlsisd.org", # Daingerfield-Lone Star ISD
                     district.num == 174908 ~ "www.centralhts.org", # Central Heights ISD
                     district.num == 178901 ~ "www.adisd.net", # Agua Dulce ISD 
                     district.num == 178913 ~ "www.banqueteisd.esc2.net", # Banquete ISD
                     district.num == 179901 ~ "www.perrytonisd.org", # Perryton ISd
                     district.num == 183801 ~ "www.panolaschools.net", # Panola Charter School
                     district.num == 184907 ~ "www.aledoisd.org", # Aledo ISD
                     district.num == 186903 ~ "isisd.esc18.net/", # Iraan-Sheffield ISD
                     district.num == 194905 ~ "www.detroiteagles.net", # Detroit ISD
                     district.num == 195901 ~ "www.pbtisd.esc18.net", # Pecos-Barstow-Toyah ISD
                     district.num == 209901 ~ "www.albanyisd.net", # Albany ISD
                     district.num == 212901 ~ "www.arpisd.org", # Arp ISD
                     district.num == 214903 ~ "www.romaisd.com", # Roma ISD
                     district.num == 219901 ~ "www.happyisd.net", # Happy ISD
                     district.num == 222901 ~ "www.terrell.esc18.net", # Terrell County ISD
                     district.num == 225906 ~ "www.chisddevils.com", # Chapel Hill ISD (2)
                     district.num == 226908 ~ "www.veribestisd.net", # Veribest ISD
                     district.num == 229906 ~ "www.chesterisd.com", # Chester ISD
                     district.num == 244905 ~ "www.northsideisd.us", # Northside ISD (2)
                     district.num == 246902 ~ "www.florenceisd.net", # Florence ISD
                     TRUE ~ district.page)) %>%
  filter(district.num != 13801) %>% # St. Mary's Academy Charter School - 404 error
  filter(district.num != 72910) %>% # Morgan Mill ISD - url doesn't work
  filter(district.num != 101872) %>% # Etoile Academy Charter School - url gives 406 error
  filter(district.num != 139908) %>% # Roxton ISD - 404 error
  filter(district.num != 143901) %>% # Hallettsville ISD - url doesn't work
  filter(district.num != 215901) %>% # Breckenridge ISD - timeout
  filter(district.num != 252902) # Newcastle ISD - 401 error


# get url list
basic_sites <- district_info %>% pull(district.page) %>% as.list()

#### LONG-ASS SCRAPING LOOPS ####

# long-ass loop: first list ####
scraped_sites <- list()
for(url in basic_sites){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}

scraped_sites_001 <- scraped_sites # 152

# second list ####

basic_sites_02 <- basic_sites[153:1163]

scraped_sites <- list()
for(url in basic_sites_02){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}

scraped_sites_002 <- scraped_sites #23

# third list ####

basic_sites_03 <- basic_sites_02[24:1011]

scraped_sites <- list()
for(url in basic_sites_03){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}

scraped_sites_003 <- scraped_sites # 90

# fourth list ####

basic_sites_04 <- basic_sites_03[91:988]

scraped_sites <- list()
for(url in basic_sites_04){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}

scraped_sites_004 <- scraped_sites

# fifth list ####

basic_sites_05 <- basic_sites_04[34:898]

scraped_sites <- list()
for(url in basic_sites_05){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}

scraped_sites_005 <- scraped_sites

# sixth list ####

basic_sites_06 <- basic_sites_05[65:865]

scraped_sites <- list()
for(url in basic_sites_06){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}

scraped_sites_006 <- scraped_sites

# seventh list ####

basic_sites_07 <- basic_sites_06[59:801]

scraped_sites <- list()
for(url in basic_sites_07){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}

test.page <- read_html("https://www.navasotaisd.org")
test_text <- test.page %>%
  html_nodes("div") %>%
  html_text() %>%
  as.list()

test_list <- c(scraped_sites, list(test_text))

scraped_sites_007 <- test_list

# eighth list ####

basic_sites_08 <- basic_sites_07[26:743]

scraped_sites <- list()
for(url in basic_sites_08){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}

scraped_sites_008 <- scraped_sites

# ninth list ####

basic_sites_09 <- basic_sites_08[46:718]

scraped_sites <- list()
for(url in basic_sites_09){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}

scraped_sites_009 <- scraped_sites

# tenth list ####

basic_sites_10 <- basic_sites_09[17:673]

scraped_sites <- list()
for(url in basic_sites_10){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}

scraped_sites_010 <- scraped_sites

# eleventh list ####

basic_sites_11 <- basic_sites_10[35:657]

scraped_sites <- list()
for(url in basic_sites_11){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}


test.page <- read_html("https://www.mcallenisd.org")
test_text <- test.page %>%
  html_nodes("div") %>%
  html_text() %>%
  as.list()

test_list <- c(scraped_sites, list(test_text))

scraped_sites_011 <- test_list

# twelfth list ####

basic_sites_12 <- basic_sites_11[8:623]

scraped_sites <- list()
for(url in basic_sites_12){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}

scraped_sites_012 <- scraped_sites

# thirteenth list ####

basic_sites_13 <- basic_sites_12[23:616]

scraped_sites <- list()
for(url in basic_sites_13){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}

scraped_sites_013 <- scraped_sites

# fourteenth list ####

basic_sites_14 <- basic_sites_13[25:594]

scraped_sites <- list()
for(url in basic_sites_14){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}

scraped_sites_014 <- scraped_sites

# fifteenth list ####

basic_sites_15 <- basic_sites_14[94:570]

scraped_sites <- list()
for(url in basic_sites_15){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}

scraped_sites_015 <- scraped_sites

# sixteenth list ####

basic_sites_16 <- basic_sites_15[12:477]

scraped_sites <- list()
for(url in basic_sites_16){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}

scraped_sites_016 <- scraped_sites

# seventeenth list ####

basic_sites_17 <- basic_sites_16[61:466]

scraped_sites <- list()
for(url in basic_sites_17){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}

test.page <- read_html("https://www.lorenaisd.net")
test_text <- test.page %>%
  html_nodes("div") %>%
  html_text() %>%
  as.list()

test_list <- c(scraped_sites, list(test_text))

scraped_sites_017 <- test_list

# eighteenth list ####

basic_sites_18 <- basic_sites_17[7:406]

scraped_sites <- list()
for(url in basic_sites_18){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}



scraped_sites_018 <- test_list

# nineteenth list ####

basic_sites_19 <- basic_sites_18[8:400]

scraped_sites <- list()
for(url in basic_sites_19){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}

test.page <- read_html("http://loraine.esc14.net/")
test_text <- test.page %>%
  html_nodes("div") %>%
  html_text() %>%
  as.list()

test_list <- c(scraped_sites, list(test_text))

scraped_sites_019 <- test_list

# twentieth list ####

basic_sites_20 <- basic_sites_19[28:393]

scraped_sites <- list()
for(url in basic_sites_20){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}

scraped_sites_020 <- scraped_sites

# twenty-first list ####

basic_sites_21 <- basic_sites_20[48:366]

scraped_sites <- list()
for(url in basic_sites_21){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}

scraped_sites_021 <- scraped_sites

# twenty-second list ####

basic_sites_22 <- basic_sites_21[10:319]

scraped_sites <- list()
for(url in basic_sites_22){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}

scraped_sites_022 <- scraped_sites

# twenty-third list ####

basic_sites_23 <- basic_sites_22[20:310]

scraped_sites <- list()
for(url in basic_sites_23){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}

scraped_sites_023 <- scraped_sites

# twenty-fourth list ####

basic_sites_24 <- basic_sites_23[86:291]

scraped_sites <- list()
for(url in basic_sites_24){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}

scraped_sites_024 <- scraped_sites

# twenty-fifth list ####

basic_sites_25 <- basic_sites_24[13:206]

scraped_sites <- list()
for(url in basic_sites_25){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}

scraped_sites_025 <- scraped_sites

# twenty-sixth list ####

basic_sites_26 <- basic_sites_25[13:194]

scraped_sites <- list()
for(url in basic_sites_26){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}

scraped_sites_026 <- scraped_sites

# twenty-seventh list ####

basic_sites_27 <- basic_sites_26[159:182]

scraped_sites <- list()
for(url in basic_sites_27){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}

scraped_sites_027 <- scraped_sites

# twenty-eighth list ####

basic_sites_28 <- basic_sites_27[21:24]

scraped_sites <- list()
for(url in basic_sites_28){
  dummy <- list()
  
  tryCatch(
    {final_url <- paste0("https://", url)
    dist.page <- read_html(final_url)},
    error = function(e) 
    {final_url <- paste0("http://", url)
    dist.page <- read_html(final_url)})
  
  body_text <- dist.page %>%
    html_nodes("div") %>%
    html_text() %>%
    as.list()
  dummy <- list(body_text)
  scraped_sites <- c(scraped_sites, dummy)
}

scraped_sites_028 <- scraped_sites

#### SAVE ####

all_scraped_sites <- c(scraped_sites_001, scraped_sites_002, scraped_sites_003, 
                       scraped_sites_004, scraped_sites_005, scraped_sites_006,
                       scraped_sites_007, scraped_sites_008, scraped_sites_009,
                       scraped_sites_010, scraped_sites_011, scraped_sites_012,
                       scraped_sites_013, scraped_sites_014, scraped_sites_015,
                       scraped_sites_016, scraped_sites_017, scraped_sites_018,
                       scraped_sites_019, scraped_sites_020, scraped_sites_021,
                       scraped_sites_022, scraped_sites_023, scraped_sites_024,
                       scraped_sites_025, scraped_sites_026, scraped_sites_027,
                       scraped_sites_028)

saveRDS(all_scraped_sites, here("output", "scraped_homepages.rds"))

all_unique_homes <- all_scraped_sites[!duplicated(all_scraped_sites)]

saveRDS(all_unique_homes, here("output", "unique_homepages.rds"))